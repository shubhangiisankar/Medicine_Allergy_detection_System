# -*- coding: utf-8 -*-
"""bappaMoryaa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1unzsRktDHPaQfQYVvNyibi4mbeoIeNmE
"""

from google.colab import files
uploaded = files.upload()
print(uploaded)

import os
os.listdir()

import pandas as pd
df = pd.read_csv("new_medicine_dataset.csv", encoding='ISO-8859-1')
df.head(100)

import pandas as pd

# Load the CSV, treating empty cells as empty strings instead of NaN
df = pd.read_csv("new_medicine_dataset.csv", encoding='ISO-8859-1', keep_default_na=False)

# Drop columns that are completely empty (i.e., all values are blank)
df = df.dropna(axis=1, how='all')

# Drop rows that are completely empty (i.e., all values are blank)
df = df.dropna(axis=0, how='all')

# Save the cleaned DataFrame to a new CSV file
df.to_csv("cleaned_medicine_dataset.csv", index=False, encoding='ISO-8859-1')

print("✅ CSV cleaned and saved as 'cleaned_medicine_dataset.csv'")

print(df)

print(df.isna().sum())

print(df.columns)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('cleaned_medicine_dataset.csv', encoding='ISO-8859-1')

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Encode categorical columns
df['name'] = label_encoder.fit_transform(df['name'])
df['substitute1'] = label_encoder.fit_transform(df['substitute1'])
df['substitute2'] = label_encoder.fit_transform(df['substitute2'])
df['substitute3'] = label_encoder.fit_transform(df['substitute3'])
df['symptoms'] = label_encoder.fit_transform(df['symptoms'])
df['symptoms.1'] = label_encoder.fit_transform(df['symptoms.1'])

df['composition1'] = label_encoder.fit_transform(df['composition1'])
df['composition2'] = label_encoder.fit_transform(df['composition2'])
df['composition3'] = label_encoder.fit_transform(df['composition3'])
df['composition4'] = label_encoder.fit_transform(df['composition4'])

# Fill any missing values with 0
df.fillna(0, inplace=True)

# Save the resulting DataFrame to a new CSV file
df.to_csv('numerical_medicine_dataset.csv', index=False)

# Display the first 15 rows of the modified DataFrame
print(df.head(15))

X = df.drop('substitute1', axis=1)
y = df['substitute1']  # Target
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(data=df)
plt.show()

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df_cleaned = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]
print(f"Original dataset size: {df.shape}")
print(f"Cleaned dataset size: {df_cleaned.shape}")

from scipy.stats import zscore
z_scores = zscore(df.select_dtypes(include=['float64', 'int64']))
df_cleaned = df[(abs(z_scores) < 3).all(axis=1)]
print(f"Original dataset size: {df.shape}")
print(f"Cleaned dataset size: {df_cleaned.shape}")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test_scaled)
print(y_pred)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Error: {mse}")
print(f"R² Score: {r2}")

plt.scatter(y_test, y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted')
plt.show()

param_grid = {
    'n_estimators': [100],
    'max_depth': [10, 20],
    'min_samples_split': [2],
    'min_samples_leaf': [1]
}

from sklearn.model_selection import RandomizedSearchCV

random_search = RandomizedSearchCV(
    rf_model,
    param_distributions=param_grid,
    n_iter=10,
    cv=3,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_train_scaled, y_train)
print("Best parameters from random search:", random_search.best_params_)

best_rf_model = random_search.best_estimator_
best_rf_model.fit(X_train_scaled, y_train)
y_pred_best = best_rf_model.predict(X_test_scaled)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, y_pred_best)
mse = mean_squared_error(y_test, y_pred_best)
r2 = r2_score(y_test, y_pred_best)

print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"R² Score: {r2}")

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

gb_model = GradientBoostingRegressor(random_state=42)

gb_model.fit(X_train_scaled, y_train)
y_pred_gb = gb_model.predict(X_test_scaled)

mae = mean_absolute_error(y_test, y_pred_gb)
mse = mean_squared_error(y_test, y_pred_gb)
r2 = r2_score(y_test, y_pred_gb)

print("Gradient Boosting Regressor Results:")
print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"R² Score: {r2}")

from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
df_poly = poly.fit_transform(df)

# The transformed dataset now has polynomial features.
# Check the shape of the transformed data to understand how many features you have.
print(df_poly.shape)

# Create a DataFrame to view the transformed features (optional, for better understanding)
df_poly_df = pd.DataFrame(df_poly, columns=poly.get_feature_names_out(df.columns))
print(df_poly_df.head())

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

# Use RFE for feature selection
selector = RFE(LinearRegression(), n_features_to_select=10)  # Select top 10 features
df_selected = selector.fit_transform(df_poly, y)  # 'y' is your target variable

import seaborn as sns
import matplotlib.pyplot as plt

corr_matrix = pd.DataFrame(df_poly).corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

from sklearn.model_selection import train_test_split
X_train_poly, X_test_poly, y_train, y_test = train_test_split(df_poly, y, test_size=0.3, random_state=42)

from sklearn.ensemble import RandomForestRegressor
rf_model_poly = RandomForestRegressor(random_state=42)
rf_model_poly.fit(X_train_poly, y_train)

y_pred_poly = rf_model_poly.predict(X_test_poly)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae_poly = mean_absolute_error(y_test, y_pred_poly)
mse_poly = mean_squared_error(y_test, y_pred_poly)
r2_poly = r2_score(y_test, y_pred_poly)

print("Random Forest with Polynomial Features Results:")
print(f"MAE: {mae_poly}")
print(f"MSE: {mse_poly}")
print(f"R² Score: {r2_poly}")

df_poly = poly.fit_transform(df)
print(df_poly.shape)

new_data = [[
    'Dolo 650',         # name
    'Paracetamol',      # composition1
    'Caffeine',         # composition2
    'Starch',           # composition3
    'Magnesium Stearate', # composition4
    'Crocin',           # substitute1
    'Calpol',           # substitute2
    'Metacin',          # substitute3
    'Fever',            # symptoms
    'Body Pain'         # symptoms.1
]]

import pandas as pd
from sklearn.preprocessing import OrdinalEncoder
used_features = ['composition1', 'composition2', 'composition3', 'composition4',
                 'substitute1', 'substitute2', 'substitute3', 'symptoms', 'symptoms.1']

# Step 2: New sample input with only those 9 features
new_data = [[
    'Paracetamol',       # composition1
    'Caffeine',          # composition2
    'Starch',            # composition3
    'Magnesium Stearate',# composition4
    'Crocin',            # substitute1
    'Calpol',            # substitute2
    'Metacin',           # substitute3
    'Fever',             # symptoms
    'Body Pain'          # symptoms.1
]]

new_data_df = pd.DataFrame(new_data, columns=used_features)
encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
encoder.fit(df[used_features])

new_data_encoded = encoder.transform(new_data_df)

new_predictions = best_rf_model.predict(new_data_encoded)

print("✅ Final prediction for new data:", new_predictions)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
le.fit(df['substitute1'])
predicted_label = le.inverse_transform([int(round(new_predictions[0]))])

print("Predicted substitute medicine:", predicted_label)

import pandas as pd
train_data = pd.read_csv('new_medicine_dataset.csv', encoding='ISO-8859-1')
print(train_data.columns)

def model_prediction(new_data, encoder, model, le=None, poly=None):
    import numpy as np
    import pandas as pd

    # Convert to DataFrame
    new_df = pd.DataFrame(new_data, columns=X.columns)  # X = your original training DataFrame

    # Encode categorical features
    new_encoded = encoder.transform(new_df)

    # Apply polynomial transformation if used
    if poly is not None:
        new_encoded = poly.transform(new_encoded)

    # Predict
    prediction = model.predict(new_encoded)

    # Inverse transform prediction if label encoder is provided
    if le is not None:
        prediction_label = le.inverse_transform([int(round(prediction[0]))])
        return prediction_label[0]
    else:
        return prediction[0]

new_data_1 = [['Dolo 650', 'Paracetamol', 'None', 'None', 'None',
               'Calpol 650', 'Pacimol 650', 'Metacin', 'Fever']]
new_data_2 = [['Aspirin', 'Acetylsalicylic Acid', 'None', 'None', 'None',
               'Disprin', 'Ascriptin', 'Bufferin', 'Pain']]

columns = ['composition1', 'composition2', 'composition3', 'composition4',
           'substitute1', 'substitute2', 'substitute3', 'symptoms', 'symptoms.1']

new_data_1_df = pd.DataFrame(new_data_1, columns=columns)
new_data_2_df = pd.DataFrame(new_data_2, columns=columns)

import pandas as pd

# Sample training data — replace this with your actual dataset later
data = pd.DataFrame([
    ['Dolo 650', 'Paracetamol', 'None', 'None', 'None', 'Calpol 650', 'Pacimol 650', 'Metacin', 'Fever', 1],
    ['Aspirin', 'Acetylsalicylic Acid', 'None', 'None', 'None', 'Disprin', 'Ascriptin', 'Bufferin', 'Pain', 0],
    ['Ibuprofen', 'Brufen', 'None', 'None', 'None', 'Advil', 'Motrin', 'Nurofen', 'Pain', 0],
], columns=[
    'composition1', 'composition2', 'composition3', 'composition4',
    'substitute1', 'substitute2', 'substitute3', 'symptoms', 'symptoms.1', 'target'
])

# Separate features and target
X = data.drop('target', axis=1)
y = data['target']

from sklearn.preprocessing import OrdinalEncoder
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

# Define the categorical columns
cat_columns = X.columns.tolist()

# Preprocessing: encode categorical columns
preprocessor = ColumnTransformer([
    ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_columns)
])

# Full pipeline: encoding + polynomial features + random forest
model_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('poly', PolynomialFeatures(include_bias=False)),
    ('rf', RandomForestClassifier(random_state=42))
])

# Fit the model
model_pipeline.fit(X, y)

# Define new data samples
new_data_1 = [['Dolo 650', 'Paracetamol', 'None', 'None', 'None',
               'Calpol 650', 'Pacimol 650', 'Metacin', 'Fever']]

new_data_2 = [['Aspirin', 'Acetylsalicylic Acid', 'None', 'None', 'None',
               'Disprin', 'Ascriptin', 'Bufferin', 'Pain']]

# Convert to DataFrames with correct column names
new_data_1_df = pd.DataFrame(new_data_1, columns=X.columns)
new_data_2_df = pd.DataFrame(new_data_2, columns=X.columns)

# Make predictions using the same pipeline
pred_1 = model_pipeline.predict(new_data_1_df)
pred_2 = model_pipeline.predict(new_data_2_df)

# Print predictions
print("Prediction 1:", pred_1)
print("Prediction 2:", pred_2)

from sklearn.preprocessing import LabelEncoder

# Assuming you encoded your target like this:
le = LabelEncoder()
le.fit(y)  # y is your original target labels during training

# Now decode the predictions
print("Prediction 1:", le.inverse_transform(pred_1))
print("Prediction 2:", le.inverse_transform(pred_2))

from sklearn.preprocessing import LabelEncoder

# Suppose this was your original column used during training
original_substitutes = df['substitute1']  # Make sure this has all original values

# Fit the LabelEncoder again on the same column
le = LabelEncoder()
le.fit(original_substitutes)

# Decode predicted numbers to actual substitute names
decoded_pred_1 = le.inverse_transform(pred_1)
decoded_pred_2 = le.inverse_transform(pred_2)

print("Prediction 1 (Substitute):", decoded_pred_1[0])
print("Prediction 2 (Substitute):", decoded_pred_2[0])

# Assuming you have your training dataset 'df'
substitute1_medicines = df['substitute2'].unique()

# Print out the unique medicine names from 'substitute1'
print(substitute1_medicines)

# Exclude the first row (where 'name' is stored)
df_cleaned = df[df['substitute2'] != 'name']

# Get the unique medicines in the 'substitute1' column
unique_medicines = df_cleaned['substitute2'].unique()

# Get the count of unique medicines
num_unique_medicines = len(unique_medicines)

# Output the result
print(f"Number of unique medicines in 'substitute2' (excluding 'name'): {num_unique_medicines}")
print("List of unique medicines:", unique_medicines)

# Exclude the first row (where 'name' is stored)
df_cleaned = df[df['substitute3'] != 'name']

# Get the unique medicines in the 'substitute1' column
unique_medicines = df_cleaned['substitute3'].unique()

# Get the count of unique medicines
num_unique_medicines = len(unique_medicines)

# Output the result
print(f"Number of unique medicines in 'substitute3' (excluding 'name'): {num_unique_medicines}")
print("List of unique medicines:", unique_medicines)

"""**Step 1: Data Preparation and Splitting for substitute2,3,4**"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Assuming you have already loaded the dataset into a DataFrame `df`
# Drop the first row if it contains the header "name" in substitute columns
df = df[df['substitute1'] != 'name']

# Features and targets
X = df.drop(columns=['substitute1', 'substitute2', 'substitute3'])  # Features (all columns except substitutes)
y_substitute2 = df['substitute2']  # Target (substitute2)
y_substitute3 = df['substitute3']  # Target (substitute3)

# Split data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train_substitute2, y_test_substitute2 = train_test_split(X, y_substitute2, test_size=0.2, random_state=42)
_, _, y_train_substitute3, y_test_substitute3 = train_test_split(X, y_substitute3, test_size=0.2, random_state=42)

print("Data preparation and splitting done.")

# Strip any leading/trailing spaces in column names
df.columns = df.columns.str.strip()

# Check the column names again to make sure they are clean
print(df.columns)

# Try accessing the 'substitute1' column directly
print(df['substitute1'].head())  # Check if you can access the values

# Check the shape of the dataframe
print(df.shape)

# Check for any missing values in 'substitute1'
print(df['substitute1'].isnull().sum())  # Check for NaN values in the column

# Check the data type of 'substitute1'
print(df['substitute1'].dtype)

# Create a small test dataset with the 'substitute1' column
test_data = pd.DataFrame({
    'substitute1': ['Medicine A', 'Medicine B', 'Medicine C'],
    'substitute2': ['Substitute A', 'Substitute B', 'Substitute C']
})

# Check if accessing 'substitute1' works
print(test_data['substitute1'].head())

"""**Step 2.1: Create the Model Pipeline for substitute2**"""

# Extracting data for substitute2 and substitute3 (removing substitute1 and substitute4 columns)
X_substitute2 = df.drop(columns=['substitute1', 'substitute3'])
y_substitute2 = df['substitute2']

X_substitute3 = df.drop(columns=['substitute1', 'substitute2'])
y_substitute3 = df['substitute3']

# Check the shapes of the datasets to ensure they are correct
print(X_substitute2.shape, y_substitute2.shape)
print(X_substitute3.shape, y_substitute3.shape)

# Strip any leading or trailing spaces in column names
df.columns = df.columns.str.strip()

# Verify the column names
print(df.columns)

# Check if substitute1 is in the dataframe, it shouldn't be in this block anymore
print('substitute1' in df.columns)  # Should return False if it is no longer present

# Ensure that substitute2 and substitute3 exist
print('substitute2' in df.columns)  # Should return True if it exists
print('substitute3' in df.columns)  # Should return True if it exists

# Create X and y for substitute2, avoid referencing substitute1 or substitute4
X_substitute2 = df.drop(columns=['substitute3'])  # Drop substitute3 for predicting substitute2
y_substitute2 = df['substitute2']

X_substitute3 = df.drop(columns=['substitute2'])  # Drop substitute2 for predicting substitute3
y_substitute3 = df['substitute3']

# Check the shapes to make sure X and y are correct
print(X_substitute2.shape, y_substitute2.shape)
print(X_substitute3.shape, y_substitute3.shape)

print("Accessing substitute3 directly:")
print(df['substitute3'].head())  # Should print the first few rows

# Check for NaN and unique values in 'substitute3'
print(f"NaN values in 'substitute3': {df['substitute3'].isna().sum()}")
print(f"Unique values in 'substitute3': {df['substitute3'].unique()}")

# Checking and cleaning up the column selection step
X_substitute3 = df[['substitute3']]  # Ensure 'substitute3' column is correctly selected
y_substitute3 = df['substitute3']   # Assuming we want to predict 'substitute3'

# Ensure the data is selected correctly before passing into the pipeline
print(f"Shape of X_substitute3: {X_substitute3.shape}")
print(f"Shape of y_substitute3: {y_substitute3.shape}")

# Check the unique values and their types
print(df['substitute3'].unique())
print(df['substitute3'].dtype)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

# Apply Label Encoding to 'substitute3'
df['substitute3'] = label_encoder.fit_transform(df['substitute3'])

# Check the result
print(df['substitute3'].head(80))
print(df['substitute3'].dtype)  # Should now be numeric (int)

y_substitute3 = label_encoder.fit_transform(df['substitute3'])

from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Load the data
df = pd.read_csv('new_medicine_dataset.csv', encoding='ISO-8859-1')

# Ensure substitute3 is numeric after label encoding
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
df['substitute3'] = label_encoder.fit_transform(df['substitute3'])

# Select features and target variable
X_substitute3 = df[['substitute3']]  # Features (input)
y_substitute3 = df['substitute3']   # Target (output)

# Sample the data (optional but faster)
df_sampled = df.sample(frac=0.1, random_state=42)  # Use 10% of the data for faster execution
X_substitute3 = df_sampled[['substitute3']]
y_substitute3 = df_sampled['substitute3']

# Create a simple model with faster training settings
rf = RandomForestClassifier(n_estimators=10, max_depth=5, n_jobs=-1)  # 10 trees, shallow trees, use all cores

# Train the model
rf.fit(X_substitute3, y_substitute3)

# Check the accuracy of the model
train_accuracy = rf.score(X_substitute3, y_substitute3)
print(f"Training accuracy: {train_accuracy:.2f}")

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# STEP 1: Load dataset
print("Loading CSV file...")
df = pd.read_csv('new_medicine_dataset.csv', encoding='ISO-8859-1')
print("CSV loaded. Shape:", df.shape)

# STEP 2: Check columns available
print("\nColumns in dataset:")
print(df.columns.tolist())

# STEP 3: Define only columns that actually exist
features = ['name', 'substitute1', 'substitute2']
target = 'substitute3'

# Check if all necessary columns are present
missing = [col for col in features + [target] if col not in df.columns]
if missing:
    print(f"\nMissing columns in dataset: {missing}")
else:
    # STEP 4: Handle missing values
    df = df[features + [target]].fillna('Unknown')

    # STEP 5: Encode all necessary columns
    label_encoder = LabelEncoder()
    for col in features + [target]:
        df[col] = label_encoder.fit_transform(df[col].astype(str))

    # STEP 6: Split the data
    X = df[features]
    y = df[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # STEP 7: Train the model
    print("\nTraining the Random Forest model...")
    model = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42)
    model.fit(X_train, y_train)

    # STEP 8: Make predictions and evaluate
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\n✅ Test Accuracy: {accuracy:.2f}")

import pandas as pd
import numpy as np

# Load your dataset
df = pd.read_csv('new_medicine_dataset.csv', encoding='ISO-8859-1', low_memory=False)

# Drop duplicates on 'name' to avoid repetition
df_unique = df[['name']].drop_duplicates().copy()

# Set seed for reproducibility
np.random.seed(42)

# Assign random safety flags to each unique medicine
df_unique['safe_for_pregnancy'] = np.random.choice([True, False], size=len(df_unique))
df_unique['safe_for_elderly'] = np.random.choice([True, False], size=len(df_unique))
df_unique['safe_for_heart_patients'] = np.random.choice([True, False], size=len(df_unique))
df_unique['safe_for_kidney_patients'] = np.random.choice([True, False], size=len(df_unique))
df_unique['safe_for_female'] = np.random.choice([True, False], size=len(df_unique))
df_unique['safe_for_male'] = np.random.choice([True, False], size=len(df_unique))

# Function to merge safety info for a given column
def merge_safety_info(df_main, df_safety, column_name):
    return df_main.merge(df_safety, how='left', left_on=column_name, right_on='name', suffixes=('', f'_{column_name}'))

# Map safety flags for each medicine column
for col in ['name', 'substitute1', 'substitute2', 'substitute3']:
    df = merge_safety_info(df, df_unique, col)

# Optional: drop extra 'name_y' columns created during merge
df = df.loc[:, ~df.columns.duplicated()]

# Show a sample
cols_to_show = [c for c in df.columns if 'safe' in c]
print(df[['name', 'substitute1', 'substitute2', 'substitute3'] + cols_to_show].head(30))

import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('numerical_medicine_dataset.csv', encoding='ISO-8859-1', low_memory=False)

# Drop duplicates based on 'name' column
df_unique = df[['name']].drop_duplicates().copy()

# Show actual number of unique medicines
print(f"Number of unique medicines: {len(df_unique)}")

# Set seed and assign random safety flags (0 or 1)
np.random.seed(42)
for col in ['safe_for_pregnancy', 'safe_for_elderly', 'safe_for_heart_patients',
            'safe_for_kidney_patients', 'safe_for_female', 'safe_for_male']:
    df_unique[col] = np.random.choice([0, 1], size=len(df_unique))

# Fill NaNs in substitute columns with 'Unknown'
for col in ['substitute1', 'substitute2', 'substitute3']:
    df[col] = df[col].fillna('Unknown')

# Add a default row for 'Unknown' medicines
unknown_row = pd.DataFrame([{
    'name': 'Unknown',
    'safe_for_pregnancy': 0,
    'safe_for_elderly': 0,
    'safe_for_heart_patients': 0,
    'safe_for_kidney_patients': 0,
    'safe_for_female': 0,
    'safe_for_male': 0
}])
df_unique = pd.concat([df_unique, unknown_row], ignore_index=True)

# Merge safety info with main dataset
def merge_safety_info(df_main, df_safety, col):
    return df_main.merge(
        df_safety.add_suffix(f'_{col}'),
        how='left',
        left_on=col,
        right_on=f'name_{col}'
    ).drop(columns=[f'name_{col}'])

for col in ['name', 'substitute1', 'substitute2', 'substitute3']:
    df = merge_safety_info(df, df_unique, col)

# Ensure no NaNs remain
safety_cols = [col for col in df.columns if 'safe_' in col]
df[safety_cols] = df[safety_cols].fillna(0).astype(int)

# View result
print(df[['name', 'substitute1', 'substitute2', 'substitute3'] + safety_cols].head())

# Optional: Save dataset
df.to_csv('medicine_dataset_with_safety_flags.csv', index=False)
print(df.head(30));

# Check for any missing values in the dataset
missing_values = df.isnull().sum()
print("Missing Values in each column:")
print(missing_values)

# Ensure that the safety columns only contain 0 or 1 (no NaNs)
safety_cols = [col for col in df.columns if 'safe_' in col]
invalid_values = df[safety_cols].isin([0, 1]).all()  # Check if each value is 0 or 1
print("\nAre all safety columns correctly labeled (0 or 1)?")
print(invalid_values)

# Ensure that there are no missing values in the key columns like 'name', 'substitute1', 'substitute2', 'substitute3'
key_columns = ['name', 'substitute1', 'substitute2', 'substitute3']
missing_key_columns = df[key_columns].isnull().sum()
print("\nMissing values in key columns:")
print(missing_key_columns)

df = pd.read_csv('medicine_dataset_with_safety_flags.csv', encoding='ISO-8859-1')
print(df.head(20));

import pandas as pd
df = pd.read_csv('medicine_dataset_with_safety_flags.csv', encoding='ISO-8859-1')
print(f"Number of rows in dataset: {len(df)}")

# Drop any unwanted index columns
df = df.drop(columns=['Unnamed: 0'], errors='ignore')

df = pd.read_csv('new_medicine_dataset.csv', encoding='ISO-8859-1', index_col=None)

# Strip spaces from column names
df.columns = df.columns.str.strip()

# Check again the first few rows
print(df.head())

# After removing duplicates, check again
print(f"Number of duplicate rows after cleaning: {df.duplicated().sum()}")

# Before dropping duplicates
print(f"Shape of the dataset before dropping duplicates: {df.shape}")

# After dropping duplicates
df = df.drop_duplicates()
print(f"Shape of the dataset after dropping duplicates: {df.shape}")
# Save the cleaned dataset to a new CSV file
df.to_csv('cleaned_medicine_dataset.csv', index=False)

print("Cleaned dataset has been saved as 'cleaned_medicine_dataset.csv'.")

print(df.columns)

import pandas as pd

df=pd.read_csv("medicine_dataset_with_safety_flags.csv");
print(df)

pip install lightgbm

import pandas as pd
import numpy as np
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
df = pd.read_csv('medicine_dataset_with_safety_flags.csv', encoding='ISO-8859-1')

# Define safety columns (replace with yours if needed)
safety_cols_substitute1 = ['safe_for_pregnancy_substitute1', 'safe_for_heart_patients_substitute1',
                           'safe_for_kidney_patients_substitute1', 'safe_for_female_substitute1', 'safe_for_male_substitute1']
safety_cols_substitute2 = ['safe_for_pregnancy_substitute2', 'safe_for_heart_patients_substitute2',
                           'safe_for_kidney_patients_substitute2', 'safe_for_female_substitute2', 'safe_for_male_substitute2']
safety_cols_substitute3 = ['safe_for_pregnancy_substitute3', 'safe_for_heart_patients_substitute3',
                           'safe_for_kidney_patients_substitute3', 'safe_for_female_substitute3', 'safe_for_male_substitute3']
safety_cols = safety_cols_substitute1 + safety_cols_substitute2 + safety_cols_substitute3

# Define features and target
X = df[['substitute1', 'substitute2', 'substitute3'] + safety_cols]
y = df['substitute3']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model using LightGBM
model = LGBMClassifier()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Accuracy - Method 1: model.score()
print(f"Accuracy using model.score(): {model.score(X_test, y_test):.2f}")

# Accuracy - Method 2: sklearn accuracy_score
print(f"Accuracy using accuracy_score(): {accuracy_score(y_test, y_pred):.2f}")

# Accuracy - Method 3: Manual calculation
manual_accuracy = (y_pred == y_test).sum() / len(y_test)
print(f"Manual accuracy: {manual_accuracy:.2f}")

# Accuracy - Method 4: NumPy way
numpy_accuracy = np.mean(y_pred == y_test)
print(f"NumPy accuracy: {numpy_accuracy:.2f}")

# Accuracy - Method 5: Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

import pandas as pd
import numpy as np
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

df = pd.read_csv('medicine_dataset_with_safety_flags.csv', encoding='ISO-8859-1', keep_default_na=False)

# Define safety columns for each substitute
safety_cols_substitute1 = ['safe_for_pregnancy_substitute1', 'safe_for_heart_patients_substitute1',
                           'safe_for_kidney_patients_substitute1', 'safe_for_female_substitute1', 'safe_for_male_substitute1']
safety_cols_substitute2 = ['safe_for_pregnancy_substitute2', 'safe_for_heart_patients_substitute2',
                           'safe_for_kidney_patients_substitute2', 'safe_for_female_substitute2', 'safe_for_male_substitute2']
safety_cols_substitute3 = ['safe_for_pregnancy_substitute3', 'safe_for_heart_patients_substitute3',
                           'safe_for_kidney_patients_substitute3', 'safe_for_female_substitute3', 'safe_for_male_substitute3']

# Combine all safety columns
safety_cols = safety_cols_substitute1 + safety_cols_substitute2 + safety_cols_substitute3

# Define features and target
X = df[['substitute1', 'substitute2', 'substitute3'] + safety_cols]
y = df['substitute3']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score
import psutil

# Check memory usage before training
print(f"Memory usage before training: {psutil.virtual_memory().percent}%")

# Load the dataset
df = pd.read_csv('medicine_dataset_with_safety_flags.csv', encoding='ISO-8859-1')

# Prepare the features and target
safety_cols_substitute1 = ['safe_for_pregnancy_substitute1', 'safe_for_heart_patients_substitute1',
                           'safe_for_kidney_patients_substitute1', 'safe_for_female_substitute1', 'safe_for_male_substitute1']
safety_cols_substitute2 = ['safe_for_pregnancy_substitute2', 'safe_for_heart_patients_substitute2',
                           'safe_for_kidney_patients_substitute2', 'safe_for_female_substitute2', 'safe_for_male_substitute2']
safety_cols_substitute3 = ['safe_for_pregnancy_substitute3', 'safe_for_heart_patients_substitute3',
                           'safe_for_kidney_patients_substitute3', 'safe_for_female_substitute3', 'safe_for_male_substitute3']

# Combine all the safety columns for substitutes
safety_cols = safety_cols_substitute1 + safety_cols_substitute2 + safety_cols_substitute3

# Prepare the feature matrix and target vector
X = df[['substitute1', 'substitute2', 'substitute3'] + safety_cols]
y = df['substitute3']

# Convert the data to float32 to save memory
X = X.astype('float32')

# Encode the target variable using LabelEncoder to ensure consistent categories
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Ensure that y_train and y_test have the same set of labels
train_labels = set(y_train)
test_labels = set(y_test)

# If there are missing labels in either train or test, ensure both sets contain the same labels
if train_labels != test_labels:
    # Ensure all missing labels are present in both
    missing_train = test_labels - train_labels
    missing_test = train_labels - test_labels

    # Add missing labels to y_train or y_test by assigning them as 0 (or a placeholder)
    if missing_train:
        print(f"Adding missing labels to y_train: {missing_train}")
        for label in missing_train:
            y_train = [label if x == label else 0 for x in y_train]

    if missing_test:
        print(f"Adding missing labels to y_test: {missing_test}")
        for label in missing_test:
            y_test = [label if x == label else 0 for x in y_test]

# Initialize the model
model = LGBMClassifier(num_leaves=31, max_depth=10, max_bin=255, n_estimators=1000, early_stopping_rounds=10)

# Train the model with early stopping
model.fit(X_train, y_train, eval_metric="logloss", eval_set=[(X_test, y_test)])

# Predict and evaluate accuracy
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Test accuracy: {accuracy:.2f}")

# Check memory usage after training
print(f"Memory usage after training: {psutil.virtual_memory().percent}%")

print("Training set label distribution:")
print(pd.Series(y_train).value_counts())

print("Test set label distribution:")
print(pd.Series(y_test).value_counts())

#model is learning to predict whether each of the 3 substitutes is safe or not for 5 types of patients:
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.multioutput import MultiOutputClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import cross_val_score

# Load your dataset (replace with the path to your data)
data = pd.read_csv('medicine_dataset_with_safety_flags.csv')

# Define the features (X) - excluding the target columns
X = data.drop(
    ['safe_for_pregnancy_substitute1', 'safe_for_heart_patients_substitute1',
     'safe_for_kidney_patients_substitute1', 'safe_for_female_substitute1', 'safe_for_male_substitute1',
     'safe_for_pregnancy_substitute2', 'safe_for_heart_patients_substitute2',
     'safe_for_kidney_patients_substitute2', 'safe_for_female_substitute2', 'safe_for_male_substitute2',
     'safe_for_pregnancy_substitute3', 'safe_for_heart_patients_substitute3',
     'safe_for_kidney_patients_substitute3', 'safe_for_female_substitute3', 'safe_for_male_substitute3'], axis=1
)  # Features

# Define the target columns (safety columns for different substitutes)
y = data[
    ['safe_for_pregnancy_substitute1', 'safe_for_heart_patients_substitute1',
     'safe_for_kidney_patients_substitute1', 'safe_for_female_substitute1', 'safe_for_male_substitute1',
     'safe_for_pregnancy_substitute2', 'safe_for_heart_patients_substitute2',
     'safe_for_kidney_patients_substitute2', 'safe_for_female_substitute2', 'safe_for_male_substitute2',
     'safe_for_pregnancy_substitute3', 'safe_for_heart_patients_substitute3',
     'safe_for_kidney_patients_substitute3', 'safe_for_female_substitute3', 'safe_for_male_substitute3'
] ] # Target columns

# Encode target variables (if necessary, as they might be categorical)
label_encoder = LabelEncoder()

# Encoding all target columns
for column in y.columns:
    y[column] = label_encoder.fit_transform(y[column])

# Define the LightGBM model
model = LGBMClassifier(max_bin=255)

# Create a MultiOutputClassifier for multi-output classification
multi_target_model = MultiOutputClassifier(model, n_jobs=-1)

# Perform cross-validation
cv_scores = cross_val_score(multi_target_model, X, y, cv=5)  # 5-fold cross-validation

# Output the cross-validation scores
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean cross-validation score: {cv_scores.mean():.2f}")

from sklearn.model_selection import train_test_split
from lightgbm import LGBMClassifier
from sklearn.model_selection import RandomizedSearchCV
import pandas as pd
import numpy as np

# Define target column as 'safe_for_pregnancy_substitute1'
target_column = 'safe_for_pregnancy_substitute1'

# Assuming the rest of the columns are features
X = data.drop(target_column, axis=1)  # Features
y = data[target_column]               # Target column (safe_for_pregnancy_substitute1)
#(80% for training, 20% for testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = LGBMClassifier()

# Define the parameter grid for RandomizedSearchCV
param_dist = {
    'num_leaves': np.arange(10, 200, 10),   # Number of leaves in the tree
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate
    'n_estimators': [100, 200, 300, 400],    # Number of boosting rounds
    'max_depth': [3, 5, 7, 10],               # Maximum depth of the tree
    'min_child_samples': [20, 30, 50],        # Minimum number of samples in a leaf
    'subsample': [0.6, 0.8, 1.0],             # Subsample ratio for training data
    'colsample_bytree': [0.6, 0.8, 1.0],      # Column sampling by tree
    'reg_alpha': [0, 0.1, 0.5],
    'reg_lambda': [0, 0.1, 0.5]
}

# Set up RandomizedSearchCV
random_search = RandomizedSearchCV(
    model,
    param_distributions=param_dist,
    n_iter=50,                  # Number of random parameter combinations to try
    scoring='accuracy',         # Evaluation metric
    cv=5,                       # 5-fold cross-validation
    verbose=1,                  # Display progress
    random_state=42             # Set random state for reproducibility
)

# Fit RandomizedSearchCV
random_search.fit(X_train, y_train)

# Get the best parameters and score
print(f"Best Parameters: {random_search.best_params_}")
print(f"Best Score: {random_search.best_score_}")

# Evaluate the model on the test set with the best parameters
best_model = random_search.best_estimator_
test_accuracy = best_model.score(X_test, y_test)
print(f"Test Set Accuracy: {test_accuracy}")

# Example: If the composition is split across multiple columns like ingredient_1, ingredient_2, etc.
composition_columns = ['composition1','composition2','composition3']  # Adjust based on your dataset
data['composition'] = data[composition_columns].apply(lambda row: ', '.join(row.values.astype(str)), axis=1)

# Now, extract the relevant columns again
medicine_compositions = data[['name', 'composition']]
print(medicine_compositions.head())

import pandas as pd
from sklearn.preprocessing import LabelEncoder
df=pd.read_csv("cleaned_medicine_dataset.csv", encoding='ISO-8859-1')
# Assuming "df" is your original DataFrame with all the columns
# Initialize encoders
label_encoder_1 = LabelEncoder()
label_encoder_2 = LabelEncoder()
label_encoder_3 = LabelEncoder()

# Fit and transform each composition column in the original dataframe "df"
df['composition1'] = label_encoder_1.fit_transform(df['composition1'])  # Changed "data" to "df"
df['composition2'] = label_encoder_2.fit_transform(df['composition2'])  # Changed "data" to "df"
df['composition3'] = label_encoder_3.fit_transform(df['composition3'])  # Changed "data" to "df"

# Decode each encoded composition column separately
df['decoded_comp1'] = label_encoder_1.inverse_transform(df['composition1'])
df['decoded_comp2'] = label_encoder_2.inverse_transform(df['composition2'])
df['decoded_comp3'] = label_encoder_3.inverse_transform(df['composition3'])

# Replace NaN values with an empty string or some placeholder
df['decoded_comp1'] = df['decoded_comp1'].fillna('')
df['decoded_comp2'] = df['decoded_comp2'].fillna('')
df['decoded_comp3'] = df['decoded_comp3'].fillna('')

# Join all three back into a single string column like before
df['decoded_composition'] = df[['decoded_comp1', 'decoded_comp2', 'decoded_comp3']].apply(lambda row: ', '.join(row.astype(str)), axis=1)

# Final result with name and decoded composition
medicine_compositions = df[['name', 'decoded_composition']]
print(medicine_compositions.head())



"""**Enter medicine name and get it's compositions**"""

def get_composition(medicine_name):
    # Check if the medicine name exists in the dataset
    result = df[df['name'].str.contains(medicine_name, case=False, na=False)]

    if not result.empty:
        # Return the decoded composition for the matched medicine
        return result[['name', 'decoded_composition']].iloc[0]
    else:
        return f"No composition found for '{medicine_name}'"

# Example usage
medicine_name_input = input("Enter the medicine name: ")
composition_info = get_composition(medicine_name_input)
print(composition_info)

def check_allergens(user_allergens, medicine_composition):
    """
    Check if any allergens from the user are in the medicine's composition.
    Marks the medicine as 'Safe', 'Moderately Risky', or 'Highly Risky' based on the number of allergens.
    """
    # Convert composition and allergens to lowercase for case insensitive comparison
    medicine_allergens = medicine_composition.lower().split(', ')  # Assuming composition is a string of allergens
    user_allergens = [allergen.lower() for allergen in user_allergens]

    # Identify common allergens
    common_allergens = set(medicine_allergens) & set(user_allergens)

    # Determine risk level based on the number of common allergens
    if len(common_allergens) > 2:
        return "Highly Risky - Consult doctor", common_allergens
    elif len(common_allergens) > 0:
        return "Moderately Risky - Monitor symptoms", common_allergens
    else:
        return "Safe", []

def get_medicine_info_with_risk(medicine_name, user_allergens):
    """
    Function that takes medicine name and user allergens, checks the composition, and identifies the risk level.
    """
    # Check if the medicine exists in the dataset
    result = df[df['name'].str.contains(medicine_name, case=False, na=False)]

    if not result.empty:
        # Extract relevant columns
        composition = result['decoded_composition'].values[0]
        # Check for allergens
        risk_level, common_allergens = check_allergens(user_allergens, composition)

        # Return the details along with the risk level
        return {
            'name': result['name'].values[0],
            'composition': composition,
            'risk_level': risk_level,
            'common_allergens': common_allergens
        }
    else:
        return f"No information found for '{medicine_name}'"

# Example usage
medicine_name_input = input("Enter the medicine name: ")
user_allergens_input = input("Enter your allergens (comma separated): ").split(', ')

medicine_info = get_medicine_info_with_risk(medicine_name_input, user_allergens_input)

# Display result
if isinstance(medicine_info, dict):
    print(f"Medicine: {medicine_info['name']}")
    print(f"Composition: {medicine_info['composition']}")
    print(f"Risk Level: {medicine_info['risk_level']}")
    if medicine_info['common_allergens']:
        print(f"Common Allergens: {', '.join(medicine_info['common_allergens'])}")
    else:
        print("No common allergens found.")
else:
    print(medicine_info)

import pandas as pd

# Sample data loading (adjust according to your dataset's path)
df = pd.read_csv("cleaned_medicine_dataset.csv", encoding='ISO-8859-1')

# Let's assume the composition columns contain numeric codes or identifiers for ingredients
composition_columns = ['composition1', 'composition2', 'composition3']  # Adjust based on your dataset

# Convert the composition columns into one 'composition' column by combining values and treating them as strings
df['composition'] = df[composition_columns].apply(lambda row: ', '.join(row.values.astype(str)), axis=1)

# Now assuming your substitutes are in columns like 'substitute1', 'substitute2', 'substitute3'
# We'll use 'substitute1' for simplicity
df['substitute'] = df['substitute1']

# Function to check allergens and determine the risk level
def check_allergen_and_risk_level(medicine_name, allergens_input):
    # Clean the user input for medicine name and allergens (strip spaces and make lowercase)
    medicine_name = medicine_name.strip().lower()
    allergens_input = [allergen.strip().lower() for allergen in allergens_input.split(',')]

    # Search for the entered medicine
    medicine_info = df[df['name'].str.lower() == medicine_name]  # Match in a case-insensitive way

    if medicine_info.empty:
        return "Medicine not found in the dataset."

    # Retrieve the composition and substitute (handle missing values if necessary)
    composition = medicine_info['composition'].values[0] if 'composition' in medicine_info.columns else ''
    substitute = medicine_info['substitute'].values[0] if 'substitute' in medicine_info.columns else 'No substitute found'

    # Clean and split the composition into a list (handle commas and spaces)
    composition_list = [ingredient.strip().lower() for ingredient in composition.split(',')] if composition else []

    # Check for common allergens
    common_allergens = set(allergens_input).intersection(composition_list)
    allergens_count = len(common_allergens)

    # Determine risk level
    if allergens_count == 0:
        risk_level = "Safe"
        risk_message = "No common allergens found."
    elif allergens_count == 1:
        risk_level = "Moderately Safe"
        risk_message = "Monitor symptoms."
    elif allergens_count >= 2:
        risk_level = "Highly Risky"
        risk_message = "Consult doctor."

    # Prepare the result
    result = {
        'Medicine': medicine_name,
        'Composition': composition,
        'Risk Level': risk_level,
        'Risk Message': risk_message,
        'Substitute': substitute
    }

    return result

# Example of user input
medicine_name = input("Enter the medicine name: ")
allergens_input = input("Enter your allergens (comma separated): ")

# Get the result
result = check_allergen_and_risk_level(medicine_name, allergens_input)

# Print the result
print(result)

import pandas as pd

# Load the dataset
df = pd.read_csv("medicine_dataset_with_safety_flags.csv", encoding='ISO-8859-1')

# Normalize column names
df.columns = df.columns.str.strip().str.lower()

# Merge composition fields into a single string
composition_columns = ['composition1', 'composition2', 'composition3']
df['composition'] = df[composition_columns].astype(str).apply(lambda row: ', '.join(row.values), axis=1)

# Primary substitute
df['substitute'] = df['substitute1'].astype(str)

# Safety and substitute columns
safety_columns = [
    'safe_for_pregnancy_name', 'safe_for_elderly_name', 'safe_for_heart_patients_name',
    'safe_for_kidney_patients_name', 'safe_for_female_name', 'safe_for_male_name'
]

substitute_columns = [
    'safe_for_pregnancy_substitute1', 'safe_for_elderly_substitute1', 'safe_for_heart_patients_substitute1',
    'safe_for_kidney_patients_substitute1', 'safe_for_female_substitute1', 'safe_for_male_substitute1'
]

# Main function
def check_allergen_and_safety(medicine_name, allergens_input):
    # Normalize input
    medicine_name = str(medicine_name).strip().lower()
    allergens_input = [str(a).strip().lower() for a in allergens_input.split(',')]

    # Ensure 'name' column is string and lowercase
    df['name'] = df['name'].astype(str).str.strip().str.lower()
    matched = df[df['name'] == medicine_name]

    if matched.empty:
        return f"❌ Medicine '{medicine_name}' not found in the dataset."

    info = matched.iloc[0]

    # Composition analysis
    composition = info['composition']
    comp_list = [c.strip().lower() for c in composition.split(',') if c]
    common_allergens = set(comp_list).intersection(allergens_input)
    allergen_count = len(common_allergens)

    # Risk level
    if allergen_count == 0:
        risk = "✅ Safe"
        msg = "No common allergens found."
    elif allergen_count == 1:
        risk = "⚠️ Moderately Safe"
        msg = "Monitor symptoms."
    else:
        risk = "❌ Highly Risky"
        msg = "Consult doctor."

    # Report Header
    print("\n💊 Medicine Safety Report:\n")
    print(f"🔹 Medicine: {info['name'].title()}")
    print(f"🔹 Composition: {composition}")
    print(f"🔹 Risk Level: {risk}")
    print(f"🔹 Risk Message: {msg}")
    print(f"🔹 General Substitute: {info['substitute'] if info['substitute'] else 'None'}\n")

    # Safety flags
    emojis = ["👶", "👵", "❤️", "🧠", "♀️", "♂️"]
    labels = ["Pregnancy", "Elderly", "Heart Patients", "Kidney Patients", "Female", "Male"]

    for i in range(len(safety_columns)):
        flag = safety_columns[i]
        sub_col = substitute_columns[i]
        safe = "Yes" if info[flag] == 1 else "No"
        substitute = info[sub_col] if pd.notna(info[sub_col]) and info[sub_col] != "" else "No substitute found"
        print(f"{emojis[i]} Safe for {labels[i]}: {'✅ Yes' if safe == 'Yes' else '❌ No'}")
        if safe == "No":
            print(f"   ➡️ Recommended Substitute: {substitute}")

# Input prompts
medicine_input = input("Enter the medicine name: ")
allergens_input = input("Enter your allergens (comma separated): ")

# Run check
check_allergen_and_safety(medicine_input, allergens_input)

import pandas as pd
df = pd.read_csv("medicine_dataset_with_safety_flags.csv", encoding='ISO-8859-1')
df.columns = df.columns.str.strip().str.lower()
composition_columns = ['composition1', 'composition2', 'composition3', 'composition4']
df[composition_columns] = df[composition_columns].astype(str)
df['composition'] = df[composition_columns].apply(lambda row: ', '.join(row), axis=1)
df['substitute'] = df['substitute1'].astype(str)

safety_columns = [
    'safe_for_pregnancy_name', 'safe_for_elderly_name', 'safe_for_heart_patients_name',
    'safe_for_kidney_patients_name', 'safe_for_female_name', 'safe_for_male_name'
]

substitute_columns = [
    'safe_for_pregnancy_substitute1', 'safe_for_elderly_substitute1', 'safe_for_heart_patients_substitute1',
    'safe_for_kidney_patients_substitute1', 'safe_for_female_substitute1', 'safe_for_male_substitute1'
]

# Main function
def check_allergen_and_risk_level(medicine_id, allergens_input):
    try:
        medicine_id = str(medicine_id).strip()
        allergens_input = [allergen.strip() for allergen in allergens_input.split(',') if allergen.strip().isdigit()]

        # Look up medicine by ID
        medicine_info = df[df['name'].astype(str) == medicine_id]
        if medicine_info.empty:
            return "Medicine not found in the dataset."
        composition = medicine_info['composition'].values[0]
        composition_list = [item.strip() for item in composition.split(',')]

        common_allergens = set(allergens_input).intersection(composition_list)
        allergens_count = len(common_allergens)

        if allergens_count == 0:
            risk_level = "Safe"
            risk_message = "No common allergens found."
        elif allergens_count == 1:
            risk_level = "Moderately Safe"
            risk_message = "Monitor symptoms."
        else:
            risk_level = "Highly Risky"
            risk_message = "Consult a doctor."
        safety_info = {}
        for safety_col, substitute_col in zip(safety_columns, substitute_columns):
            is_safe = medicine_info[safety_col].values[0]
            safety_info[safety_col] = "Yes" if is_safe == 1 else "No"
            sub_val = medicine_info[substitute_col].values[0]
            safety_info[substitute_col] = str(int(sub_val)) if not pd.isna(sub_val) else "No substitute found"

        result = {
            'Medicine ID': medicine_id,
            'Composition (IDs)': composition,
            'Risk Level': risk_level,
            'Risk Message': risk_message,
            'Substitute (ID)': medicine_info['substitute'].values[0]
        }

        result.update(safety_info)
        return result
    except Exception as e:
        return f"Error: {str(e)}"
medicine_id = input("Enter the Medicine ID: ")
allergens_input = input("Enter allergen ingredient IDs (comma separated): ")

result = check_allergen_and_risk_level(medicine_id, allergens_input)
print(result)

import pandas as pd

# Load dataset
df = pd.read_csv("cleaned_medicine_dataset.csv", encoding='ISO-8859-1')

# Normalize column names
df.columns = df.columns.str.strip().str.lower()

# Convert numeric columns to string for processing
composition_columns = ['composition1', 'composition2', 'composition3', 'composition4']
df[composition_columns] = df[composition_columns].astype(str)

# Combine all composition parts into a single string
df['composition'] = df[composition_columns].apply(lambda row: ', '.join(row), axis=1)

# Ensure substitute is also treated as string
df['substitute'] = df['substitute1'].astype(str)

# Define safety columns
safety_columns = [
    'safe_for_pregnancy_name', 'safe_for_elderly_name', 'safe_for_heart_patients_name',
    'safe_for_kidney_patients_name', 'safe_for_female_name', 'safe_for_male_name'
]

substitute_columns = [
    'safe_for_pregnancy_substitute1', 'safe_for_elderly_substitute1', 'safe_for_heart_patients_substitute1',
    'safe_for_kidney_patients_substitute1', 'safe_for_female_substitute1', 'safe_for_male_substitute1'
]

# Main function
def check_allergen_and_risk_level(medicine_name, allergens_input):
    try:
        # Convert inputs
        medicine_name = medicine_name.strip().lower()
        allergens_input = [allergen.strip().lower() for allergen in allergens_input.split(',') if allergen.strip()]

        # Look up medicine by name (case-insensitive match)
        medicine_info = df[df['name'].str.lower() == medicine_name]
        if medicine_info.empty:
            return "Medicine not found in the dataset."

        # Extract composition
        composition = medicine_info['composition'].values[0]
        composition_list = [item.strip().lower() for item in composition.split(',')]

        # Check for allergen match
        common_allergens = set(allergens_input).intersection(composition_list)
        allergens_count = len(common_allergens)

        if allergens_count == 0:
            risk_level = "Safe"
            risk_message = "No common allergens found."
        elif allergens_count == 1:
            risk_level = "Moderately Safe"
            risk_message = "Monitor symptoms."
        else:
            risk_level = "Highly Risky"
            risk_message = "Consult a doctor."

        # Safety info
        safety_info = {}
        for safety_col, substitute_col in zip(safety_columns, substitute_columns):
            is_safe = medicine_info[safety_col].values[0]
            safety_info[safety_col] = "Yes" if is_safe == 1 else "No"
            sub_val = medicine_info[substitute_col].values[0]
            safety_info[substitute_col] = str(int(sub_val)) if not pd.isna(sub_val) else "No substitute found"

        result = {
            'Medicine': medicine_name,
            'Composition': composition,
            'Risk Level': risk_level,
            'Risk Message': risk_message,
            'Substitute': medicine_info['substitute'].values[0]
        }

        result.update(safety_info)
        return result
    except Exception as e:
        return f"Error: {str(e)}"

# Example input
medicine_name = input("Enter the medicine name: ")
allergens_input = input("Enter your allergens (comma separated): ")

result = check_allergen_and_risk_level(medicine_name, allergens_input)
print(result)

import pandas as pd

# Load dataset
df = pd.read_csv("cleaned_medicine_dataset.csv", encoding='ISO-8859-1')

# Normalize column names (to ensure consistent formatting)
df.columns = df.columns.str.strip().str.lower()

# Convert numeric columns to string for processing
composition_columns = ['composition1', 'composition2', 'composition3', 'composition4']
df[composition_columns] = df[composition_columns].astype(str)

# Combine all composition parts into a single string
df['composition'] = df[composition_columns].apply(lambda row: ', '.join(row), axis=1)

# Ensure substitute is also treated as string
df['substitute'] = df['substitute1'].astype(str)

# Define the correct column names for safety checks based on the dataset you provided
safety_columns = [
    'safe_for_heart_patients_substitute2',
    'safe_for_kidney_patients_substitute2',
    'safe_for_female_substitute2',
    'safe_for_male_substitute2',
    'safe_for_pregnancy_substitute3'
]

# Main function with debugging statements
def check_allergen_and_risk_level(medicine_name, allergens_input):
    try:
        # Normalize the input
        medicine_name = medicine_name.strip().lower()
        allergens_input = [allergen.strip().lower() for allergen in allergens_input.split(',') if allergen.strip()]

        print(f"Normalized medicine name: {medicine_name}")
        print(f"Normalized allergens: {allergens_input}")

        # Look up the medicine in the dataset
        medicine_info = df[df['name'].str.lower() == medicine_name]

        # Debugging statement to check if the medicine is found
        print(f"Matching entries for medicine '{medicine_name}':")
        print(medicine_info)

        if medicine_info.empty:
            return "Medicine not found in the dataset."

        # Extract the composition
        composition = medicine_info['composition'].values[0]
        composition_list = [item.strip().lower() for item in composition.split(',')]

        # Check for allergens
        common_allergens = set(allergens_input).intersection(composition_list)
        allergens_count = len(common_allergens)

        print(f"Common allergens found: {common_allergens}")

        if allergens_count == 0:
            risk_level = "Safe"
            risk_message = "No common allergens found."
        elif allergens_count == 1:
            risk_level = "Moderately Safe"
            risk_message = "Monitor symptoms."
        else:
            risk_level = "Highly Risky"
            risk_message = "Consult a doctor."

        # Safety checks for different conditions
        safety_info = {}
        for safety_col in safety_columns:
            is_safe = medicine_info[safety_col].values[0]
            safety_info[safety_col] = "Yes" if is_safe == 1 else "No"

        # Prepare result
        result = {
            'Medicine': medicine_name,
            'Composition': composition,
            'Risk Level': risk_level,
            'Risk Message': risk_message,
            'Substitute': medicine_info['substitute'].values[0]
        }

        result.update(safety_info)
        return result

    except Exception as e:
        return f"Error: {str(e)}"

# Example input
medicine_name = input("Enter the medicine name: ")
allergens_input = input("Enter your allergens (comma separated): ")

result = check_allergen_and_risk_level(medicine_name, allergens_input)
print(result)

import pandas as pd

# Load dataset with dtype specification to avoid mixed types warning
df = pd.read_csv("medicine_dataset_with_safety_flags.csv", encoding='ISO-8859-1')

# Strip column names to remove any leading/trailing spaces
df.columns = df.columns.str.strip()

# Display all column names to verify
print("Columns in the dataset:")
print(df.columns)

# Convert numeric composition columns to string for processing
composition_columns = ['composition1', 'composition2', 'composition3', 'composition4']
df[composition_columns] = df[composition_columns].astype(str)

# Combine composition parts into a single string
df['composition'] = df[composition_columns].apply(lambda row: ', '.join(row), axis=1)

# Ensure substitute is treated as a string
df['substitute'] = df['substitute1'].astype(str)

# Define safety check columns
safety_columns = [
    'safe_for_heart_patients_substitute2',
    'safe_for_kidney_patients_substitute2',
    'safe_for_female_substitute2',
    'safe_for_male_substitute2',
    'safe_for_pregnancy_substitute3'
]

# Check if these columns exist in the dataset
missing_columns = [col for col in safety_columns if col not in df.columns]
if missing_columns:
    print(f"Missing columns: {missing_columns}")
else:
    print("All safety columns are present.")

# Main function for allergen matching and risk level calculation
def check_allergen_and_risk_level(medicine_name, allergens_input):
    try:
        # Normalize inputs
        medicine_name = medicine_name.strip().lower()
        allergens_input = [allergen.strip().lower() for allergen in allergens_input.split(',') if allergen.strip()]

        # Look up the medicine in the dataset
        medicine_info = df[df['name'].str.lower() == medicine_name]

        if medicine_info.empty:
            return "Medicine not found in the dataset."

        # Extract the composition
        composition = medicine_info['composition'].values[0]
        composition_list = [item.strip().lower() for item in composition.split(',')]

        # Check for allergens
        common_allergens = set(allergens_input).intersection(composition_list)
        allergens_count = len(common_allergens)

        # Determine risk level based on allergens found
        if allergens_count == 0:
            risk_level = "Safe"
            risk_message = "No common allergens found."
        elif allergens_count == 1:
            risk_level = "Moderately Safe"
            risk_message = "Monitor symptoms."
        else:
            risk_level = "Highly Risky"
            risk_message = "Consult a doctor."

        # Safety checks for different conditions
        safety_info = {}
        for safety_col in safety_columns:
            if safety_col in medicine_info.columns:
                is_safe = medicine_info[safety_col].values[0]
                safety_info[safety_col] = "Yes" if is_safe == 1 else "No"
            else:
                safety_info[safety_col] = "N/A"  # Handle missing columns if any

        # Prepare the result in a structured way
        result = {
            'Medicine': medicine_name.capitalize(),
            'Composition': composition,
            'Risk Level': risk_level,
            'Risk Message': risk_message,
            'Substitute': medicine_info['substitute'].values[0]
        }

        # Add safety information to the result
        result.update(safety_info)

        # Return the result in a readable format
        output = "\n".join([f"{key}: {value}" for key, value in result.items()])
        return output

    except Exception as e:
        return f"Error: {str(e)}"

# Example input
medicine_name = input("Enter the medicine name: ")
allergens_input = input("Enter your allergens (comma separated): ")

result = check_allergen_and_risk_level(medicine_name, allergens_input)
print(result)